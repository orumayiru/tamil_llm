{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852f3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://ta.quora.com/search?q=%E0%AE%AE%E0%AF%81.%E0%AE%95%E0%AE%B0%E0%AF%81%E0%AE%A3%E0%AE%BE%E0%AE%A8%E0%AE%BF%E0%AE%A4%E0%AE%BF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac3ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 09:45:41 [py.warnings] WARNING: /tmp/ipykernel_83319/2604012682.py:12: ScrapyDeprecationWarning: scrapy.loader.processors.TakeFirst is deprecated, instantiate itemloaders.processors.TakeFirst instead.\n",
      "  question = scrapy.Field(output_processor=TakeFirst())\n",
      "\n",
      "2023-04-30 09:45:41 [py.warnings] WARNING: /tmp/ipykernel_83319/2604012682.py:13: ScrapyDeprecationWarning: scrapy.loader.processors.TakeFirst is deprecated, instantiate itemloaders.processors.TakeFirst instead.\n",
      "  answer = scrapy.Field(output_processor=TakeFirst())\n",
      "\n",
      "2023-04-30 09:45:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-04-30 09:45:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.1, Platform Linux-6.2.13-arch1-1-x86_64-with-glibc2.37\n",
      "2023-04-30 09:45:41 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
      "2023-04-30 09:45:41 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-04-30 09:45:41 [scrapy.extensions.telnet] INFO: Telnet Password: 5b2ce93d9fc4f19c\n",
      "2023-04-30 09:45:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-04-30 09:45:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-04-30 09:45:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-04-30 09:45:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-04-30 09:45:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-04-30 09:45:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-04-30 09:45:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess(settings\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSER_AGENT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     65\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(MySpider)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m reactor\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.10/site-packages/scrapy/crawler.py:383\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    381\u001b[0m tp\u001b[38;5;241m.\u001b[39madjustPoolsize(maxthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    382\u001b[0m reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m--> 383\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.10/site-packages/twisted/internet/base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.10/site-packages/twisted/internet/base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03mhandlers should be installed later.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03m    installed during startup.\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n\u001b[0;32m-> 1299\u001b[0m \u001b[43mReactorBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReactorBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.10/site-packages/twisted/internet/base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import json\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.exceptions import CloseSpider\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import TakeFirst, MapCompose\n",
    "from twisted.internet import reactor\n",
    "url = \"https://ta.quora.com/search?q=%E0%AE%AE%E0%AF%81.%E0%AE%95%E0%AE%B0%E0%AF%81%E0%AE%A3%E0%AE%BE%E0%AE%A8%E0%AE%BF%E0%AE%A4%E0%AE%BF\"\n",
    "class QAItem(scrapy.Item):\n",
    "    question = scrapy.Field(output_processor=TakeFirst())\n",
    "    answer = scrapy.Field(output_processor=TakeFirst())\n",
    "\n",
    "class MySpider(CrawlSpider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = [url]\n",
    "\n",
    "    rules = (Rule(LinkExtractor(), callback='parse_page', follow=True),)\n",
    "\n",
    "    def parse_page(self, response):\n",
    "        if response.status == 429:\n",
    "            # handle rate limiting\n",
    "            self.logger.warning('Rate limited. Retrying in 5 seconds...')\n",
    "            time.sleep(5)\n",
    "            yield scrapy.Request(response.url, callback=self.parse_page, dont_filter=True)\n",
    "            return\n",
    "\n",
    "        # scroll to bottom of page\n",
    "        driver = response.request.meta['driver']\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # wait for page to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # find all 'qt_read_more' elements and click them\n",
    "        for read_more in driver.find_elements_by_class_name('qt_read_more'):\n",
    "            read_more.click()\n",
    "\n",
    "        # find all 'qu-pb--tiny' elements\n",
    "        tiny_questions = driver.find_elements_by_class_name('qu-pb--tiny')\n",
    "\n",
    "        for tiny_question in tiny_questions:\n",
    "            # find 'puppeteer_test_question_title' and 'puppeteer_test_answer_content' elements\n",
    "            question_title = tiny_question.find_element_by_class_name('puppeteer_test_question_title')\n",
    "            answer_content = tiny_question.find_element_by_class_name('puppeteer_test_answer_content')\n",
    "\n",
    "            # extract text from elements and store in 'QAItem'\n",
    "            loader = ItemLoader(item=QAItem(), selector=tiny_question)\n",
    "            loader.add_value('question', question_title.text)\n",
    "            loader.add_value('answer', answer_content.text)\n",
    "            yield loader.load_item()\n",
    "\n",
    "    def closed(self, reason):\n",
    "        # save data to JSON file\n",
    "        items = self.crawler.stats.get_value('item_scraped_count')\n",
    "        if items:\n",
    "            filename = f'{self.name}.json'\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(items, f, indent=2)\n",
    "if not reactor.running:\n",
    "    reactor.run()\n",
    "if __name__ == '__main__':\n",
    "    process = CrawlerProcess(settings={\n",
    "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'})\n",
    "    process.crawl(MySpider)\n",
    "    process.start()\n",
    "\n",
    "    reactor.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "puppeteer_test_question_title spacing_log_answer_content puppeteer_test_answer_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6499bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    " .qt_read_more  qu-pb--tiny puppeteer_test_question_title puppeteer_test_answer_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
